{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "README\n",
        "\n",
        "*   change the file path to your directory where you store the datasets and models.\n",
        "*   for only showing the results, skip the training cell (# run the entire pipeline) and go to result cell(# results showing)\n",
        "*   together with this script,two best models are uploaded (best_model_state_0.82.bin) for original pap dataset  and (best_model_state_0.84.bin) for data augmented dataset, which add pep3k data into original data. load the one you wish to see the results in results cell.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PvOBB6Ay08zG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKWbWkMAO0Kj"
      },
      "outputs": [],
      "source": [
        "# load google drive in colab for reading data and model save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install library requirements\n",
        "!pip install transformers torch pandas"
      ],
      "metadata": {
        "id": "Bdd0gzDJz0Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n"
      ],
      "metadata": {
        "id": "Ik25kAXPO3wH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset class\n",
        "class SemanticDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "BUbEMCxLO3ys"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter configuration\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 2e-5\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n"
      ],
      "metadata": {
        "id": "JuWKAg7RO31k"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data loader function\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size,mode=1):\n",
        "    if mode==1:\n",
        "      label_mapping = {'plausible': 0,'implausible': 1}\n",
        "      df['label_num'] = df['original_label'].map(label_mapping)\n",
        "    ds = SemanticDataset(\n",
        "        texts=df.text.to_numpy(),\n",
        "        labels=df.label_num.to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n"
      ],
      "metadata": {
        "id": "GT5CSImPO34a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluate function\n",
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double() / n_examples, sum(losses) / len(losses)\n"
      ],
      "metadata": {
        "id": "hEODdK5yO37U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function\n",
        "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    Trained_sample_count = 0\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        labels = d[\"labels\"].to(device)\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        Trained_sample_count+=BATCH_SIZE\n",
        "\n",
        "        loss = outputs.loss\n",
        "        correct_predictions += torch.sum(outputs.logits.argmax(1) == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if Trained_sample_count%50==0:\n",
        "          print(\"finished:\",Trained_sample_count,\"accuracy:\",correct_predictions.double()/Trained_sample_count)\n",
        "\n",
        "    return correct_predictions.double() / n_examples, sum(losses) / len(losses)\n"
      ],
      "metadata": {
        "id": "bHnpz1oGO39z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main training loop\n",
        "def train_model(model, train_data_loader, val_data_loader, device, n_epochs):\n",
        "    best_accuracy = 0\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(f'Epoch {epoch + 1}/{n_epochs}')\n",
        "        train_acc, train_loss = train_epoch(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            optimizer,\n",
        "            device,\n",
        "            len(train_df)\n",
        "        )\n",
        "        print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "        val_acc, val_loss = eval_model(\n",
        "            model,\n",
        "            val_data_loader,\n",
        "            device,\n",
        "            len(val_df)\n",
        "        )\n",
        "        print(f'Validation loss {val_loss} accuracy {val_acc}')\n",
        "\n",
        "        if val_acc > best_accuracy:\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/pap/best_model_state.bin')\n",
        "            best_accuracy = val_acc\n",
        "            print('Saved Best Model')\n",
        "\n",
        "    print ('model training finished')"
      ],
      "metadata": {
        "id": "2ZisXu4FO4Am"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "def evaluate(model, test_data_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    predictions, true_labels = [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert to class predictions\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "    # Calculate the average loss\n",
        "    avg_loss = total_loss / len(test_data_loader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(true_labels, predictions)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n"
      ],
      "metadata": {
        "id": "Ml7yBgBrO4DY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "\n",
        "\n",
        "# 1 original training data: with pap dataset\n",
        "#train_df = pd.read_csv('/content/drive/MyDrive/pap/train.csv')\n",
        "#val_df = pd.read_csv('/content/drive/MyDrive/pap/dev.csv')\n",
        "#test_df = pd.read_csv('/content/drive/MyDrive/pap/test.csv')\n",
        "\n",
        "# 2 augmented training data: add pep3k data into original pap dataset\n",
        "train_df1 = pd.read_csv('/content/drive/MyDrive/pap/train.csv')\n",
        "train_df2 = pd.read_csv('/content/drive/MyDrive/pep3k/train.csv')\n",
        "label_mapping = {1:'plausible',0:'implausible'}\n",
        "train_df2['original_label'] = train_df2['label'].map(label_mapping)\n",
        "train_df2 = train_df2[['text', 'original_label']]\n",
        "train_df1 = train_df1[['text', 'original_label']]\n",
        "\n",
        "train_df = train_df1.append(train_df2)\n",
        "train_df.sample(frac=1).reset_index(drop=True)\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/pap/dev.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/pap/test.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAAInPm-mfuA",
        "outputId": "12e1a813-134f-4636-ea62-db23f911fb31"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-88f1c17e817d>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  train_df = train_df1.append(train_df2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the entire pipeline\n",
        "try:\n",
        "    train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "    test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/pap/best_model_state_0.84.bin'))\n",
        "    # model.load_state_dict(torch.load('/content/drive/MyDrive/pap/best_model_state_0.82.bin'))\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_model(model, train_data_loader, val_data_loader, device, EPOCHS)\n",
        "    evaluate(model, test_data_loader, device)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "iqK6_xtRKnUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results showing\n",
        "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "model_temp = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_temp.load_state_dict(torch.load('/content/drive/MyDrive/pap/best_model_state_0.84.bin')) # to see best result with pap and pep3k dataset\n",
        "# model_temp.load_state_dict(torch.load('/content/drive/MyDrive/pap/best_model_state_0.82.bin')) # to see best result with original pap dataset\n",
        "model_temp = model_temp.to(device)\n",
        "\n",
        "evaluate(model_temp, test_data_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBUHkrLXh78d",
        "outputId": "d77bd93d-1609-4b65-e8d0-8704f47bc327"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.4608197945424102\n",
            "Accuracy: 0.8390804597701149\n",
            "Precision: 0.8072289156626506\n",
            "Recall: 0.8481012658227848\n"
          ]
        }
      ]
    }
  ]
}